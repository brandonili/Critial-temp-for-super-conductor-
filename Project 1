import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, KFold, cross_val_predict
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import make_pipeline

# Load the datasets
train_df = pd.read_csv('train.csv')
unique_m_df = pd.read_csv('unique_m.csv')

# Explore the first few rows of each dataframe
print(train_df.head())
print(unique_m_df.head())

# Statistical summary and visualization
print(train_df.describe())
sns.histplot(train_df['critical_temp'], kde=True)
plt.title('Critical Temp')
plt.show()

# Check for missing values and duplicates
print(train_df.isnull().sum())
print(unique_m_df.isnull().sum())

train_df = train_df.drop_duplicates()
unique_m_df = unique_m_df.drop_duplicates()

# Feature selection using SelectKBest
X = train_df.iloc[:, :-1]  # all columns except the last one
y = train_df['critical_temp']  # last column

selector = SelectKBest(score_func=f_regression, k=10)
X_new = selector.fit_transform(X, y)

# Model selection and N-fold cross-validation
model = RandomForestRegressor()
cv = KFold(n_splits=10, random_state=1, shuffle=True)
scores = cross_val_score(model, X_new, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=None)

#scores = cross_val_score(model, X_new, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)

# Report performance
print('MAE: %.3f (%.3f)' % (-scores.mean(), scores.std()))

# Visualization of Model Performance
# Fitting model for feature importances
model.fit(X_new, y)
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plotting feature importances
plt.figure()
plt.title('Feature Importances')
plt.bar(range(X_new.shape[1]), importances[indices], color="r", align="center")
plt.xticks(range(X_new.shape[1]), indices)
plt.xlim([-1, X_new.shape[1]])
plt.show()

# Scatter plot of actual vs predicted values
predicted = cross_val_predict(model, X_new, y, cv=cv)
plt.scatter(y, predicted, edgecolors=(0, 0, 0))
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.show()

# Line graph of cross-validation scores over N-folds
mean_score = -scores.mean()  # Removed axis argument
std_dev = scores.std()  # Removed axis argument
plt.errorbar(range(1, len(scores) + 1), [mean_score]*len(scores), yerr=[std_dev]*len(scores))
plt.xlabel('Fold')
plt.ylabel('Mean Absolute Error')
plt.title('Cross-validation Score over N-folds')
plt.show()


